
####################### from machineLearning.py #######################

def makeSVMClassifier(method, base_estimator, num_folds, param_grid, df, labels, train_data, variance_explained):
    
    print()
    print(f"Classifier: \t {base_estimator}")
    print(f"Optimalizer: \t {method}")
    print("-" * 40)

    start_time = time.time()

    if method.lower() == 'manualgridsearchcv':
      
      # Unpack dictionary into lists
      C_list, kernel_types, gamma_list, coef0_list, deg_list = [list(values) for values in param_grid.values()]

      hyperparams_list = []

      # Initialize arrays for evaluating score = (mean - std)
      metrics_matrix = np.zeros( (num_folds, len(C_list), len(kernel_types), len(gamma_list), len(coef0_list), len(deg_list)) )
      metrics_matrix_mean = np.zeros( (len(C_list), len(kernel_types), len(gamma_list), len(coef0_list), len(deg_list)) )
      metrics_matrix_std = np.zeros( (len(C_list), len(kernel_types), len(gamma_list), len(coef0_list), len(deg_list)) )

      ''' K-FOLD SPLIT '''

      skf = StratifiedKFold(n_splits = num_folds)

      ''' HYPERPARAMETER OPTIMIZATION '''
      for i, (train_index, test_index) in enumerate(skf.split(train_data, labels)):

        print(f"PCA fitting on fold {i}")
          
        # Debug prints
        # print(f"  Train: index={train_index}")
        # print(f"  Test:  index={test_index}")
        # print(f"Train labels: {train_labels}")

        kfold_train_labels = [labels[j] for j in train_index]
        kfold_test_labels = [labels[j] for j in test_index]

        # unique, counts = np.unique(kfold_train_labels, return_counts=True)
        # print(dict(zip(unique, counts)))

        # print(kfold_testLabels)
        
        kfold_train_data = train_data.iloc[train_index]
        kfold_validation_data = train_data.iloc[test_index]

        # Scale training and validation separately
        scaler = StandardScaler()
        scaler.set_output(transform="pandas")

        kfold_train_data_scaled = scaler.fit_transform(kfold_train_data)
        kfold_validation_data_scaled = scaler.transform(kfold_validation_data)

        PCA_components = setNComponents(kfold_train_data_scaled, variance_explained=variance_explained)
        
        PCA_fold = PCA(n_components = PCA_components)
        
        kfold_PCA_train_df = pd.DataFrame(PCA_fold.fit_transform(kfold_train_data_scaled))
        kfold_PCA_validation_df = pd.DataFrame(PCA_fold.transform(kfold_validation_data_scaled))

        # if (want_plots):
        #   print(f"Plotting PCA plots for fold {i}")
        #   biplot(kfold_PCA_train_df, kfold_train_labels, PCA_fold, PCA_components, separate_types)


        for j, C_value in enumerate(C_list):

            for k, kernel in enumerate(kernel_types):
                    
                # print("Work in progress")

              if kernel == 'linear':
                l, m, n = 0, 0, 0   
                clf = svm.SVC(C=C_value, kernel=kernel)
                clf.fit(kfold_PCA_train_df, kfold_train_labels)
                test_predict = clf.predict(kfold_PCA_validation_df)
                # accuracy_array[i, j, k, :, :, :] = 0
                metrics_matrix[i, j, k, l, m, n] = metrics.f1_score(kfold_test_labels, test_predict, average="micro")
                
                # Only append for fold 0
                if i == 0:
                    hyperparams_list.append((C_value, kernel))

              elif kernel == 'poly':
                      
                for l, gamma_value in enumerate(gamma_list):
                    for m, coef0_value in enumerate(coef0_list):
                        for n, deg_value in enumerate(deg_list):
                          # print(f"Working on {j} {k} {l} {m} {n}")
                          clf = svm.SVC(C=C_value, kernel=kernel, gamma=gamma_value, coef0=coef0_value, degree=deg_value)
                          clf.fit(kfold_PCA_train_df, kfold_train_labels)
                          test_predict = clf.predict(kfold_PCA_validation_df)
                          metrics_matrix[i, j, k, l, m, n] = metrics.f1_score(kfold_test_labels, test_predict, average="micro")
                          
                          if i == 0:
                              hyperparams_list.append((C_value, kernel, gamma_value, coef0_value, deg_value))

              elif kernel == 'sigmoid': 
                  
                for l, gamma_value in enumerate(gamma_list):
                    for m, coef0_value in enumerate(coef0_list):    
                        n = 0
                        clf = svm.SVC(C=C_value, kernel=kernel, gamma=gamma_value, coef0=coef0_value)
                        clf.fit(kfold_PCA_train_df, kfold_train_labels)
                        test_predict = clf.predict(kfold_PCA_validation_df)
                        metrics_matrix[i, j, k, l, m, n] = metrics.f1_score(kfold_test_labels, test_predict, average="micro")   

                        if i == 0:
                            hyperparams_list.append((C_value, kernel, gamma_value, coef0_value))

              elif kernel == 'rbf':

                for l, gamma_value in enumerate(gamma_list):
                    m, n = 0, 0
                    clf = svm.SVC(C=C_value, kernel=kernel, gamma=gamma_value)
                    clf.fit(kfold_PCA_train_df, kfold_train_labels)
                    test_predict = clf.predict(kfold_PCA_validation_df)
                    metrics_matrix[i, j, k, l, m, n] = metrics.f1_score(kfold_test_labels, test_predict, average="micro")  

                    if i == 0:
                        hyperparams_list.append((C_value, kernel, gamma_value))

      print(f"Created {len(hyperparams_list) * num_folds} SVM classifiers.")
      print("\n")

      # Exhaustive grid search: calculate which hyperparams gives highest score = max|mean - std|
      for j in range(len(C_list)):
          for k in range(len(kernel_types)):
              for l in range(len(gamma_list)):
                  for m in range(len(coef0_list)):
                      for n in range(len(deg_list)):
                          metrics_matrix_mean[j, k, l, m, n] = metrics_matrix[:, j, k, l, m, n].mean()
                          metrics_matrix_std[j, k, l, m, n] = metrics_matrix[:, j, k, l, m, n].std()

      score_matrix = metrics_matrix_mean - metrics_matrix_std

      # Find location and value of highest score
      max_value_index = np.argmax(score_matrix)
      max_value = np.max(score_matrix)
      multi_dim_index = np.unravel_index(max_value_index, score_matrix.shape)
      # print(score_array.shape)
      # print(best_param)
      # print(len(multi_dim_index))

      # Unknown error !!!!!!!!!!!!!!! ^ must be investigated

      '''
      Tror problemet ligger i at score_array er en cube med mange 0 verdier
      0 for alle verdier som ikke settes i loopen (degree 2,3,4 for 'linear' fr.eks)
      Potensielt bevis: score_array uten alle 0 verdier er like lang som hyper_param_list
      Videre gir multi_dim_index og hyper_param_list(best_param_test) like parametre
      '''
      # score_array_test = score_matrix.flatten()
      # score_array_test = [i for i in score_array_test if i != 0]
      # # print(score_array_1D)
      # print(f"Size of score_array_test: {len(score_array_test)}")
      # # Find location and value of highest score
      # best_param = np.argmax(score_matrix)
      # print(f"Index of best parameter, converted to 2D array (cube): {best_param}")
      # best_param_test = np.argmax(score_array_test) 
      # print(f"Index of best parameter, converted to 2D array (not cube): {best_param_test}")
      # print(f"Best combination of hyperparameters (C, kernel, gamma, coef0, degree): {hyperparams_list[best_param_test]}")
      # ''' Ser en del endringer ble gjort, legger denne her for n√•, kan slettes '''


      clf_best_params = {
          "C": C_list[multi_dim_index[0]],
          "kernel": kernel_types[multi_dim_index[1]],
          "gamma": gamma_list[multi_dim_index[2]],
          "coef0": coef0_list[multi_dim_index[3]],
          "degree": deg_list[multi_dim_index[4]]
      }
        
      print(f"All combinations of hyper params: {len(hyperparams_list)}")

      clf = svm.SVC(**clf_best_params)
      clf.fit(df, labels)

      end_time = time.time()  # End timer
      elapsed_time = end_time - start_time
      
      print(f"Best model found and fitted in {elapsed_time:.4f} seconds")
      print(f"{clf_best_params} gives the parameter setting with the highest (mean-std) score: {max_value}")
      print(f"\n")

    elif method.lower() == 'gridsearchcv':

      clf = GridSearchCV(
            estimator = base_estimator,
            param_grid = param_grid,
            scoring = 'accuracy',
            cv = num_folds, 
            verbose = 0,
            n_jobs = -1
            )
      
      clf.fit(df, labels)
      clf_best_params = clf.best_params_

      end_time = time.time()  # End timer
      elapsed_time = end_time - start_time

      best_score = max( clf.cv_results_['mean_test_score'] - clf.cv_results_['std_test_score'] )

      print(f"Best model found and fitted in {elapsed_time:.4f} seconds")
      print(f"{clf.cv_results_['params'][clf.best_index_]} gives the parameter setting with the highest (mean - std): {best_score}")
      print(f"\n")

    elif method.lower() == 'halvinggridsearchcv':
     
      clf = HalvingGridSearchCV(
            estimator = base_estimator,
            param_grid = param_grid,
            factor = 2,
            scoring = 'accuracy',
            cv = num_folds,
            verbose = 0,
            n_jobs = -1
            )
      
      clf.fit(df, labels)
      clf_best_params = clf.best_params_

      end_time = time.time()  # End timer
      elapsed_time = end_time - start_time

      best_score = max( clf.cv_results_['mean_test_score'] - clf.cv_results_['std_test_score'] )

      print(f"Best model found and fitted in {elapsed_time:.4f} seconds")
      print(f"{clf.cv_results_['params'][clf.best_index_]} gives the parameter setting with the highest (mean - std): {best_score}")
      print(f"\n")

    elif method.lower() == 'bayessearchcv':
      
      hyperparams_space = {
        "C": Real(param_grid['C'][0], param_grid['C'][-1], prior="log-uniform"),  # Continuous log-scale for C
        "kernel": Categorical(["linear", "poly", "rbf", "sigmoid"]),  # Discrete choices
        "gamma": Real(param_grid['gamma'][0], param_grid['gamma'][-1], prior="log-uniform"),  # Log-uniform scale for gamma
        "coef0": Real(param_grid['coef0'][0], param_grid['coef0'][-1]),
        "degree": Integer(param_grid['degree'][0], param_grid['degree'][-1])
      }

      clf = BayesSearchCV(
            estimator = base_estimator,
            search_spaces = hyperparams_space,
            n_iter = 30,
            scoring = 'accuracy',
            cv = num_folds,
            verbose = 0,
            n_jobs = 5
            )
      
      clf.fit(df, labels)
      clf_best_params = clf.best_params_

      end_time = time.time()  # End timer
      elapsed_time = end_time - start_time

      best_score = max( clf.cv_results_['mean_test_score'] - clf.cv_results_['std_test_score'] )

      print(f"Best model found and fitted in {elapsed_time:.4f} seconds")
      print(f"{clf.cv_results_['params'][clf.best_index_]} gives the parameter setting with the highest (mean - std): {best_score}")
      print(f"\n")

    elif method.lower() == 'randomizedsearchcv':
      
      clf = RandomizedSearchCV(
            estimator = base_estimator,
            param_distributions = param_grid,
            n_iter = 30,
            scoring = 'accuracy',
            cv = num_folds,
            verbose = 0,
            n_jobs = 5
            )
      
      clf.fit(df, labels)
      clf_best_params = clf.best_params_

      end_time = time.time()  # End timer
      elapsed_time = end_time - start_time

      best_score = max( clf.cv_results_['mean_test_score'] - clf.cv_results_['std_test_score'] )

      print(f"Best model found and fitted in {elapsed_time:.4f} seconds")
      print(f"{clf.cv_results_['params'][clf.best_index_]} gives the parameter setting with the highest (mean - std): {best_score}")
      print(f"\n")
      
    else: 
      
      clf = base_estimator
      clf.fit(df, labels)
      clf_best_params = {'C':1, 'kernel': 'rbf'}

      end_time = time.time()  # End timer
      elapsed_time = end_time - start_time

      print(f"Base model fitted in {elapsed_time:.4f} seconds")
      print(f"Optimizer {method} not recognized, using default Support Vector Classifier.")
      print(f"\n")

    return clf, clf_best_params

def makeRFClassifier(method, base_estimator, num_folds, param_grid, df, labels):
  
  print()
  print(f"Classifier: \t {base_estimator}")
  print(f"Optimalizer: \t {method}")
  print("-" * 40)
  
  start_time = time.time()
  
  if method == 'GridSearchCV':
    
    clf = GridSearchCV(estimator=base_estimator,
                       param_grid=param_grid,
                       cv=num_folds,
                       n_jobs=-1
                       ) 
    
    clf.fit(df, labels)
    clf_best_params = clf.best_params_

    end_time = time.time()  # End timer
    elapsed_time = end_time - start_time

    best_score = max( clf.cv_results_['mean_test_score'] - clf.cv_results_['std_test_score'] )

    print(f"Best model found and fitted in {elapsed_time:.4f} seconds")
    print(f"{clf.cv_results_['params'][clf.best_index_]} gives the parameter setting with the highest (mean - std): {best_score}")
    print(f"\n")

  elif method == 'HalvingGridSearchCV':

    clf = HalvingGridSearchCV(
          estimator=base_estimator,
          param_grid = param_grid,
          cv=num_folds,
          n_jobs=-1
          )
    
    clf.fit(df, labels)
    clf_best_params = clf.best_params_
    
    end_time = time.time()  # End timer
    elapsed_time = end_time - start_time

    best_score = max( clf.cv_results_['mean_test_score'] - clf.cv_results_['std_test_score'] )

    print(f"Best model found and fitted in {elapsed_time:.4f} seconds")
    print(f"{clf.cv_results_['params'][clf.best_index_]} gives the parameter setting with the highest (mean - std): {best_score}")
    print(f"\n")

  else:
    
    clf = base_estimator

    clf.fit(df, labels)
    clf_best_params = {'n_estimators': 100,
                        'criterion': 'gini',
                        'max_depth': None,
                        'min_samples_split': 2,
                        'min_samples_leaf': 1,
                        'max_features': 'sqrt'}
  
    end_time = time.time()  # End timer
    elapsed_time = end_time - start_time

    print(f"Base model fitted in {elapsed_time:.4f} seconds")
    print(f"Method {method} not recognized, fitting default Random Forest Classifier")
    print(f"\n")

  return clf, clf_best_params

def makeKNNClassifier(method, base_estimator, num_folds, param_grid, df, labels):
    
    print()
    print(f"Classifier: \t {base_estimator}")
    print(f"Optimalizer: \t {method}")
    print("-" * 40)

    start_time = time.time()

    if method.lower() == 'gridsearchcv':
        
        clf = GridSearchCV(
            estimator=base_estimator,
            param_grid=param_grid,
            cv=num_folds,
            n_jobs=-1
            ) 
        
        clf.fit(df, labels)
        clf_best_params = clf.best_params_

        end_time = time.time()  # End timer
        elapsed_time = end_time - start_time

        best_score = max( clf.cv_results_['mean_test_score'] - clf.cv_results_['std_test_score'] )

        print(f"Best model found and fitted in {elapsed_time:.4f} seconds")
        print(f"{clf.cv_results_['params'][clf.best_index_]} gives the parameter setting with the highest (mean - std): {best_score}")
        print(f"\n")

    elif method.lower() == 'halvinggridsearchcv':
        
        clf = HalvingGridSearchCV(
            estimator=base_estimator,
            param_grid=param_grid,
            cv=num_folds,
            n_jobs=-1
            ) 
        
        clf.fit(df, labels)
        clf_best_params = clf.best_params_

        end_time = time.time()  # End timer
        elapsed_time = end_time - start_time

        best_score = max( clf.cv_results_['mean_test_score'] - clf.cv_results_['std_test_score'] )

        print(f"Best model found and fitted in {elapsed_time:.4f} seconds")
        print(f"{clf.cv_results_['params'][clf.best_index_]} gives the parameter setting with the highest (mean - std): {best_score}")
        print(f"\n")

    elif method.lower() == 'randomizedsearchcv':
        
        clf = RandomizedSearchCV(
            estimator = base_estimator,
            param_distributions = param_grid,
            n_iter = 30,
            scoring = 'accuracy',
            cv = num_folds,
            verbose = 0,
            n_jobs = -1
            )
        
        clf.fit(df, labels)
        clf_best_params = clf.best_params_

        end_time = time.time()  # End timer
        elapsed_time = end_time - start_time

        best_score = max( clf.cv_results_['mean_test_score'] - clf.cv_results_['std_test_score'] )

        print(f"Best model found and fitted in {elapsed_time:.4f} seconds")
        print(f"{clf.cv_results_['params'][clf.best_index_]} gives the parameter setting with the highest (mean - std): {best_score}")
        print(f"\n")

    else:
        
        clf = KNeighborsClassifier(n_neighbors=3)

        clf.fit(df, labels)
        clf_best_params = {"n_neighbors": 3}

        end_time = time.time()  # End timer
        elapsed_time = end_time - start_time

        print(f"Base model fitted in {elapsed_time:.4f} seconds")
        print(f"Optimizer {method} not recognized, fitting default KNN model with 3 neighbors")
        print(f"\n")
    
    return clf, clf_best_params

def makeGNBClassifier(method, base_estimator, num_folds, param_grid, df, labels):
    
    print()
    print(f"Classifier: \t {base_estimator}")
    print(f"Optimalizer: \t {method}")
    print("-" * 40)
    
    if (method == "ahadhaidiahodihaj"):
        print("HOW?!")
    else:
        clf = base_estimator
        clf.fit(df, labels)

        print(clf.get_params())
    return clf

def evaluateCLF(name, clf, test_df, test_labels, want_plots, activity_name, clf_name):
    
    print(f"{name} scores")

    test_predict = clf.predict(test_df)

    accuracy_score = metrics.accuracy_score(test_labels, test_predict)
    precision_score = metrics.precision_score(test_labels, test_predict, average="weighted")
    recall_score = metrics.recall_score(test_labels, test_predict, average="weighted")
    f1_score = metrics.f1_score(test_labels, test_predict, average="weighted")

    print(f"Accuracy: \t {accuracy_score:.4f}")
    print(f"Precision: \t {precision_score:.4f}")
    print(f"Recall: \t {recall_score:.4f}")
    print(f"f1: \t\t {f1_score:.4f}")
    print("-" * 23)

    if(want_plots):
        ''' CONFUSION MATRIX '''
        conf_matrix = metrics.confusion_matrix(test_labels, test_predict, labels=activity_name)
        plt.figure(figsize=(10, 8))
        sns.heatmap(conf_matrix, annot=True, cmap='coolwarm', xticklabels=activity_name, yticklabels=activity_name)
        plt.xlabel("Predicted")
        plt.ylabel("Actual")
        plt.title(f'Confusion matrix, {clf_name}, {name}')
    
    return accuracy_score

def makeClassifier(base_estimator, param_grid, method, X, y, search_kwargs,  n_iter=30):
    
    print()
    print(f"Classifier: \t {base_estimator}")
    print(f"Optimalizer: \t {method}")
    print("-" * 40)

    start_time = time.time()

    if method.lower() == 'gridsearchcv':
        
        clf = GridSearchCV(
           
            estimator=base_estimator,
            param_grid=param_grid,
            
            **search_kwargs

            ) 

    elif method.lower() == 'halvinggridsearchcv':
        
        clf = HalvingGridSearchCV(
           
            estimator=base_estimator,
            param_grid=param_grid,
           
            **search_kwargs

            ) 
         
    elif method.lower() == 'randomizedsearchcv':
        
        clf = RandomizedSearchCV(
           
            estimator=base_estimator,
            param_distributions=param_grid,
            
            n_iter=n_iter,
            **search_kwargs

            ) 
        
    elif method.lower() == 'bayessearchcv':
        
        smooth_param_grid = makeSmoothParamGrid(param_grid)

        clf = BayesSearchCV(

            estimator=base_estimator,
            search_spaces=smooth_param_grid,
          
            n_iter=n_iter,
            **search_kwargs

            )

    else:
       clf = base_estimator
       best_params = None
       print(f"{method} not recognized, fitting default {base_estimator}")

    clf.fit(X, y)

    end_time = time.time()  # End timer
    elapsed_time = end_time - start_time

    if clf != base_estimator: 
      best_params = clf.best_params_

      best_score = ( clf.cv_results_['mean_test_score'][clf.best_index_] - clf.cv_results_['std_test_score'][clf.best_index_] )
      print(clf.best_score_)
      print(f"{clf.cv_results_['params'][clf.best_index_]} gives the parameter setting with the highest (mean - std): {best_score}")
      print(f"Best model found and fitted in {elapsed_time:.4f} seconds")
      print(f"\n")  

    return clf, best_params

####################### from plotting.py #######################

def plot_SVM_boundaries(X, train_labels, label_mapping,
                        classifiers, optimization_methods, best_clf_params, accuracy_list):

  # Check if there is actually 2 components in the clf
  if classifiers[0].n_features_in_ == 2:

    mapped_labels = np.array([label_mapping[label] for label in train_labels])

    xs, ys = X[0], X[1]
    
    fig, sub = plt.subplots(2, 2)
    plt.subplots_adjust(wspace=0.4, hspace=0.4)

    for clf, method, title, accuracy, ax in zip(classifiers, optimization_methods, best_clf_params, accuracy_list, sub.flatten()):
        
        disp = DecisionBoundaryDisplay.from_estimator(
            clf,
            X,
            response_method="predict",
            cmap=plt.cm.coolwarm,
            alpha=0.8,
            ax=ax,
            xlabel='PC1',
            ylabel='PC2',
        )
        ax.scatter(xs, ys, c=mapped_labels, cmap=plt.cm.coolwarm, s=20, edgecolors="k")
        ax.set_xticks(())
        ax.set_yticks(())
        ax.set_title(str(method) + "\n" + "Accuracy: " + str(accuracy) + "\n" + str(title) )

  else:
    print(f"Cannot plot SVM boundaries: Classifiers has {classifiers[0].n_features_in_} features, must be 2.")

def old_biplot(X, trainLabels, PCATest, n_components, separate_types, models, optimization_methods, titles, accuracy_list):
  
  # OLD BIPLOT FUNCTION, NOT IN USE
  # PCA_object = PCA(n_components = n_components)

  if n_components == 2:

    coeff = PCATest.components_.T
    labels = PCATest.feature_names_in_

    # loadings = PCATest.components_.T * np.sqrt(PCATest.explained_variance_)
    # plt.figure(figsize=(10, 8))
    # sns.heatmap(loadings, annot=True, cmap='coolwarm', xticklabels=['PC1', 'PC2'], yticklabels=PCATest.feature_names_in_)
    # plt.title('Feature Importance in Principal Components')
    
    xs, ys = X[0], X[1]

    plt.figure(figsize=(10, 8))

    if(separate_types):
      label_mapping = {'IDLE': (0.0, 0.0, 0.0)  , 
                       'GRINDBIG': (1.0, 0.0, 0.0),'GRINDMED': (1.0, 0.5, 0.0), 'GRINDSMALL': (1.0, 0.0, 0.5),
                       'SANDSIM': (0.0, 1.0, 0.0), 
                       'WELDALTIG': (0.0, 0.0, 1.0), 'WELDSTMAG': (0.5, 0.0, 1.0), 'WELDSTTIG': (0.0, 0.5, 1.0)}
    else:
      label_mapping = {'IDLE': (0.0, 0.0, 0.0)  , 'GRINDING': (1.0, 0.0, 0.0), 'SANDSIMULATED': (0.0, 1.0, 0.0), 'WELDING': (0.0, 0.0, 1.0)}

    y_labels = np.array(trainLabels)
    mappedLabels = np.array([label_mapping[label] for label in trainLabels])

    plt.scatter(xs, ys, c=mappedLabels#, cmap='viridis'
                )
    
    
    # Check if there is actually 2 components in the clf
    if models[0].n_features_in_ == 2:

      fig, sub = plt.subplots(2, 2)
      plt.subplots_adjust(wspace=0.4, hspace=0.4)

      for clf, method, title, accuracy, ax in zip(models, optimization_methods, titles, accuracy_list, sub.flatten()):
          
          disp = DecisionBoundaryDisplay.from_estimator(
              clf,
              X,
              response_method="predict",
              cmap=plt.cm.coolwarm,
              alpha=0.8,
              ax=ax,
              xlabel='PC1',
              ylabel='PC2',
          )
          ax.scatter(xs, ys, c=mappedLabels, cmap=plt.cm.coolwarm, s=20, edgecolors="k")
          ax.set_xticks(())
          ax.set_yticks(())
          ax.set_title(str(method) + "\n" + "Accuracy: " + str(accuracy) + "\n" + str(title) )


    # Uncomment if you want arrows
    # for i in range(len(coeff)):
    #     plt.arrow(0, 0, coeff[i, 0], coeff[i, 1], color='r', alpha=0.5)
    #     plt.text(coeff[i, 0] * 1.2, coeff[i, 1] * 1.2, labels[i], color='g')

    plt.xlabel("PC1")
    plt.ylabel("PC2")
    # plt.title("Biplot")
    # plt.figure()

  elif n_components == 3:

    coeff = PCATest.components_.T
    labels = PCATest.feature_names_in_

    # loadings = PCATest.components_.T * np.sqrt(PCATest.explained_variance_)
    # plt.figure(figsize=(10, 8))
    # sns.heatmap(loadings, annot=True, cmap='coolwarm', xticklabels=['PC1', 'PC2', 'PC3'], yticklabels=PCATest.feature_names_in_)
    # plt.title('Feature Importance in Principal Components')

    xs, ys, zs = X[0], X[1], X[2]

    # Create a 3D plot
    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')
    if (separate_types):
      label_mapping = {'IDLE': (0.0, 0.0, 0.0)  , 
                       'GRINDBIG': (1.0, 0.0, 0.0),'GRINDMED': (1.0, 0.5, 0.0), 'GRINDSMALL': (1.0, 0.0, 0.5),
                       'SANDSIM': (0.0, 1.0, 0.0), 
                       'WELDALTIG': (0.0, 0.0, 1.0), 'WELDSTMAG': (0.5, 0.0, 1.0), 'WELDSTTIG': (0.0, 0.5, 1.0)}
    else:
      label_mapping = {'IDLE': (0.0, 0.0, 0.0)  , 'GRINDING': (1.0, 0.0, 0.0), 'SANDSIMULATED': (0.0, 1.0, 0.0), 'WELDING': (0.0, 0.0, 1.0)}
    y_labels = np.array(trainLabels)
    mappedLabels = np.array([label_mapping[label] for label in trainLabels])

    # Create 3D scatter plot
    sc = ax.scatter(xs, ys, zs, c=mappedLabels#, cmap='inferno'
                    )

    # Draw arrows for the components
    # for i in range(len(coeff)):
    #     ax.quiver(0, 0, 0, coeff[i, 0], coeff[i, 1], coeff[i, 2], color='r', alpha=0.5)

    #     ax.text(coeff[i, 0] * 1.2, coeff[i, 1] * 1.2, coeff[i, 2] * 1.2, labels[i], color='g')

    ax.set_xlabel('PC1')
    ax.set_ylabel('PC2')
    ax.set_zlabel('PC3')
    ax.set_title("3D Biplot")
    # plt.figure()

  elif 3 < n_components < 10:

    coeff = PCATest.components_.T
    labels = PCATest.feature_names_in_

    loadings = PCATest.components_.T * np.sqrt(PCATest.explained_variance_)

    loadingsPerc = (loadings - np.min(loadings)) / (np.max(loadings) - np.min(loadings))
    plt.figure(figsize=(10, 8))
    sns.heatmap(loadingsPerc, annot=True, cmap='coolwarm', xticklabels=['PC1', 'PC2'], yticklabels=PCATest.feature_names_in_)
    plt.title('Feature Importance in Principal Components')

  else:
    print(f"Too many principal components to plot in a meaningful way")
    pass



####################### Random ################################


# label_mapping   = {
#                     'IDLE':         (0.0, 0.0, 0.0), 
#                     'GRINDBIG':     (1.0, 0.0, 0.0), 'GRINDMED':    (1.0, 0.5, 0.0), 'GRINDSMALL':  (1.0, 0.0, 0.5),
#                     'IMPA':         (0.5, 0.5, 0.5), 
#                     'SANDSIM':      (0.0, 1.0, 0.0), 
#                     'WELDALTIG':    (0.0, 0.0, 1.0), 'WELDSTMAG':   (0.5, 0.0, 1.0), 'WELDSTTIG':   (0.0, 0.5, 1.0)
# }

# label_mapping   = {'IDLE': (0.0, 0.0, 0.0), 'GRINDING': (1.0, 0.0, 0.0), 'IMPA': (0.5, 0.5, 0.5), 'SANDSIMULATED': (0.0, 1.0, 0.0), 'WELDING': (0.0, 0.0, 1.0)}
