######################## from extractFeatures.py ##############################3

def extractAllFeatures(datasets, datasetsLabel, window_length_sec, fs, Norm_Accel):
    windowLabel = []
    all_window_features = []
    windowSum = 0
    window_length = window_length_sec * fs
    
    # Renames data inside Datafiles/xxx folder

    for i, name in enumerate(datasets):
        ''' PREPROCESS FILES '''
        delete_header(name + ".txt") # Deletes lines before Timestamp and does some regex
        tab_txt_to_csv(name + ".txt", name + ".csv") # Converts from .txt to .csv

        ''' LOAD .CSV DATAFILES '''
        df = pd.read_csv(name+".csv")

        ''' REMOVE 10 SECONDS '''
        df.drop(df.index[:fs*10]) # Drop everything before 10 seconds
        df.drop(df.index[fs*10:]) # Drop everything after 10 seconds

        # 1, 2, 4, 5, or 10 data points must be selected
        time_data   = df["Timestamp"]  # Timedata, only used to measure amount of samples
        
        gyro_X      = df["Gyr.X"]  # Gyroscope data in the X direction
        gyro_Y      = df["Gyr.Y"]  # Gyroscope data in the Y direction
        gyro_Z      = df["Gyr.Z"]  # Gyroscope data in the Z direction

        accel_X     = df["Axl.X"]  # Accelerometer data in the X direction
        accel_Y     = df["Axl.Y"]  # Accelerometer data in the Y direction
        accel_Z     = df["Axl.Z"]  # Accelerometer data in the Z direction

        mag_X       = df["Mag.X"]  # Magnetometer data in the X direction
        mag_Y       = df["Mag.Y"]  # Magnetometer data in the Y direction
        mag_Z       = df["Mag.Z"]  # Magnetometer data in the Z direction

        temp        = df["Temp"]   # Ambient temperature (°C)
        # press   = df["Press"]  # Air pressure (Pa or hPa)

        # range   = df["Range"]  # Distance to object (meters)
        # lum     = df["Lum"]    # Light intensity (lux)
        # IR_lum  = df["IRLum"]  # Infrared light intensity

        num_samples = len(time_data) # Number of measurements
        
        ''' LOAD WINDOWS '''
        num_windows = num_samples // window_length # Rounds down when deciding numbers
        print(f"Number of IMU windows in {name} after cut: {num_windows}")
        windowSum += num_windows
        
        # Only does feature extraction on windows in the middle
        for j in range(0, num_windows):
            
            # Define the start and end index for the window
            start_idx = j * window_length
            end_idx = start_idx + window_length
            # print(f"Getting features from window {start_idx} to {end_idx}")  
                    
            # Windowing the signals
            window_gyro_X  = gyro_X[start_idx:end_idx]
            window_gyro_Y  = gyro_Y[start_idx:end_idx]
            window_gyro_Z  = gyro_Z[start_idx:end_idx]

            window_mag_X  = mag_X[start_idx:end_idx]
            window_mag_Y  = mag_Y[start_idx:end_idx]
            window_mag_Z  = mag_Z[start_idx:end_idx]

            window_temp = temp[start_idx:end_idx]

            if Norm_Accel == True:
                # Normalize acceleration
                
                norm_acceleration = np.sqrt( np.power(accel_X, 2) + np.power(accel_Y, 2) + np.power(accel_Z, 2))

                g_constant = np.mean(norm_acceleration)
                # print(f"g constant: {g_constant}")
                gravless_norm = np.subtract(norm_acceleration, g_constant)  
                window_accel_Norm = gravless_norm[start_idx:end_idx]

                # Get temporal features (mean, std, MAD, etc as datatype dict)
                # 10 columns, 10 Time domain features each = 100 elements
                window_features_accel_Norm_Time = get_Time_Domain_features_of_signal(window_accel_Norm, "accel_Norm")
                window_features_gyro_X_Time     = get_Time_Domain_features_of_signal(window_gyro_X, "gyro_X")
                window_features_gyro_Y_Time     = get_Time_Domain_features_of_signal(window_gyro_Y, "gyro_Y")
                window_features_gyro_Z_Time     = get_Time_Domain_features_of_signal(window_gyro_Z, "gyro_Z")
                window_features_mag_X_Time      = get_Time_Domain_features_of_signal(window_mag_X, "mag_X")
                window_features_mag_Y_Time      = get_Time_Domain_features_of_signal(window_mag_Y, "mag_Y")
                window_features_mag_Z_Time      = get_Time_Domain_features_of_signal(window_mag_Z, "mag_Z")
                window_features_temp_Time       = get_Time_Domain_features_of_signal(window_temp, "temp")

                # Get frequency features (Welch's method)
                # 10 columns, 4 Frequency domain features each = 40 elements
                window_features_accel_Norm_Freq = get_Freq_Domain_features_of_signal(window_accel_Norm, "accel_Norm", fs)
                window_features_gyro_X_Freq     = get_Freq_Domain_features_of_signal(window_gyro_X, "gyro_X", fs)
                window_features_gyro_Y_Freq     = get_Freq_Domain_features_of_signal(window_gyro_Y, "gyro_Y", fs)
                window_features_gyro_Z_Freq     = get_Freq_Domain_features_of_signal(window_gyro_Z, "gyro_Z", fs)
                window_features_mag_X_Freq      = get_Freq_Domain_features_of_signal(window_mag_X, "mag_X", fs)
                window_features_mag_Y_Freq      = get_Freq_Domain_features_of_signal(window_mag_Y, "mag_Y", fs)
                window_features_mag_Z_Freq      = get_Freq_Domain_features_of_signal(window_mag_Z, "mag_Z", fs)
                # window_features_temp_Freq       = get_Freq_Domain_features_of_signal(window_temp, "temp", fs)


                ## merge all
                window_features = {
                                **window_features_accel_Norm_Time, 
                                **window_features_accel_Norm_Freq,

                                **window_features_gyro_X_Time,
                                **window_features_gyro_Y_Time,
                                **window_features_gyro_Z_Time,
                                **window_features_gyro_X_Freq,
                                **window_features_gyro_Y_Freq,
                                **window_features_gyro_Z_Freq,

                                **window_features_mag_X_Time,
                                **window_features_mag_Y_Time,
                                **window_features_mag_Z_Time,
                                **window_features_mag_X_Freq,
                                **window_features_mag_Y_Freq,
                                **window_features_mag_Z_Freq,

                                **window_features_temp_Time,
                                # **window_features_temp_Freq
                                }
                        
            if Norm_Accel == False:

                window_accel_X = accel_X[start_idx:end_idx]
                window_accel_Y = accel_Y[start_idx:end_idx]
                window_accel_Z = accel_Z[start_idx:end_idx]

                # Get temporal features (mean, std, MAD, etc as datatype dict)
                # 10 columns, 10 Time domain features each = 100 elements
                window_features_accel_X_Time    = get_Time_Domain_features_of_signal(window_accel_X, "accel_X")
                window_features_accel_Y_Time    = get_Time_Domain_features_of_signal(window_accel_Y, "accel_Y")
                window_features_accel_Z_Time    = get_Time_Domain_features_of_signal(window_accel_Z, "accel_Z")
                window_features_gyro_X_Time     = get_Time_Domain_features_of_signal(window_gyro_X, "gyro_X")
                window_features_gyro_Y_Time     = get_Time_Domain_features_of_signal(window_gyro_Y, "gyro_Y")
                window_features_gyro_Z_Time     = get_Time_Domain_features_of_signal(window_gyro_Z, "gyro_Z")
                window_features_mag_X_Time      = get_Time_Domain_features_of_signal(window_mag_X, "mag_X")
                window_features_mag_Y_Time      = get_Time_Domain_features_of_signal(window_mag_Y, "mag_Y")
                window_features_mag_Z_Time      = get_Time_Domain_features_of_signal(window_mag_Z, "mag_Z")
                window_features_temp_Time       = get_Time_Domain_features_of_signal(window_temp, "temp")
                
                # Get frequency features from Welch's method
                # 9 columns, 4 Frequency domain features each = 36 elements
                window_features_accel_X_Freq    = get_Freq_Domain_features_of_signal(window_accel_X, "accel_X", fs)
                window_features_accel_Y_Freq    = get_Freq_Domain_features_of_signal(window_accel_Y, "accel_Y", fs)
                window_features_accel_Z_Freq    = get_Freq_Domain_features_of_signal(window_accel_Z, "accel_Z", fs)
                window_features_gyro_X_Freq     = get_Freq_Domain_features_of_signal(window_gyro_X, "gyro_X", fs)
                window_features_gyro_Y_Freq     = get_Freq_Domain_features_of_signal(window_gyro_Y, "gyro_Y", fs)
                window_features_gyro_Z_Freq     = get_Freq_Domain_features_of_signal(window_gyro_Z, "gyro_Z", fs)
                window_features_mag_X_Freq      = get_Freq_Domain_features_of_signal(window_mag_X, "mag_X", fs)
                window_features_mag_Y_Freq      = get_Freq_Domain_features_of_signal(window_mag_Y, "mag_Y", fs)
                window_features_mag_Z_Freq      = get_Freq_Domain_features_of_signal(window_mag_Z, "mag_Z", fs)
                # window_features_temp_Freq       = get_Freq_Domain_features_of_signal(window_temp, "temp", fs)


                # Merge all
                window_features = {
                                **window_features_accel_X_Time, 
                                **window_features_accel_Y_Time,
                                **window_features_accel_Z_Time,
                                **window_features_accel_X_Freq,
                                **window_features_accel_Y_Freq,
                                **window_features_accel_Z_Freq,

                                **window_features_gyro_X_Time,
                                **window_features_gyro_Y_Time,
                                **window_features_gyro_Z_Time,
                                **window_features_gyro_X_Freq,
                                **window_features_gyro_Y_Freq,
                                **window_features_gyro_Z_Freq,
                                
                                **window_features_mag_X_Time,
                                **window_features_mag_Y_Time,
                                **window_features_mag_Z_Time,
                                **window_features_mag_X_Freq,
                                **window_features_mag_Y_Freq,
                                **window_features_mag_Z_Freq,

                                **window_features_temp_Time
                                # , **window_features_temp_Freq
                                }

            # Append the features of the current window to the list
            all_window_features.append(window_features)
            windowLabel.append(datasetsLabel[i])

        print(f"Total number on IMU windows: {windowSum}") 

    # Convert the list of features to a Pandas DataFrame for easy manipulation
    feature_df = pd.DataFrame(all_window_features)

    # print(f"Total number of windows: {activityWindowsCounter}")
    return feature_df, windowLabel

####################### from machineLearning.py #######################

def makeSVMClassifier(method, base_estimator, num_folds, param_grid, df, labels, train_data, variance_explained):
    
    print()
    print(f"Classifier: \t {base_estimator}")
    print(f"Optimalizer: \t {method}")
    print("-" * 40)

    start_time = time.time()

    if method.lower() == 'manualgridsearchcv':
      
      # Unpack dictionary into lists
      C_list, kernel_types, gamma_list, coef0_list, deg_list = [list(values) for values in param_grid.values()]

      hyperparams_list = []

      # Initialize arrays for evaluating score = (mean - std)
      metrics_matrix = np.zeros( (num_folds, len(C_list), len(kernel_types), len(gamma_list), len(coef0_list), len(deg_list)) )
      metrics_matrix_mean = np.zeros( (len(C_list), len(kernel_types), len(gamma_list), len(coef0_list), len(deg_list)) )
      metrics_matrix_std = np.zeros( (len(C_list), len(kernel_types), len(gamma_list), len(coef0_list), len(deg_list)) )

      ''' K-FOLD SPLIT '''

      skf = StratifiedKFold(n_splits = num_folds)

      ''' HYPERPARAMETER OPTIMIZATION '''
      for i, (train_index, test_index) in enumerate(skf.split(train_data, labels)):

        print(f"PCA fitting on fold {i}")
          
        # Debug prints
        # print(f"  Train: index={train_index}")
        # print(f"  Test:  index={test_index}")
        # print(f"Train labels: {train_labels}")

        kfold_train_labels = [labels[j] for j in train_index]
        kfold_test_labels = [labels[j] for j in test_index]

        # unique, counts = np.unique(kfold_train_labels, return_counts=True)
        # print(dict(zip(unique, counts)))

        # print(kfold_testLabels)
        
        kfold_train_data = train_data.iloc[train_index]
        kfold_validation_data = train_data.iloc[test_index]

        # Scale training and validation separately
        scaler = StandardScaler()
        scaler.set_output(transform="pandas")

        kfold_train_data_scaled = scaler.fit_transform(kfold_train_data)
        kfold_validation_data_scaled = scaler.transform(kfold_validation_data)

        PCA_components = setNComponents(kfold_train_data_scaled, variance_explained=variance_explained)
        
        PCA_fold = PCA(n_components = PCA_components)
        
        kfold_PCA_train_df = pd.DataFrame(PCA_fold.fit_transform(kfold_train_data_scaled))
        kfold_PCA_validation_df = pd.DataFrame(PCA_fold.transform(kfold_validation_data_scaled))

        # if (want_plots):
        #   print(f"Plotting PCA plots for fold {i}")
        #   biplot(kfold_PCA_train_df, kfold_train_labels, PCA_fold, PCA_components, separate_types)


        for j, C_value in enumerate(C_list):

            for k, kernel in enumerate(kernel_types):
                    
                # print("Work in progress")

              if kernel == 'linear':
                l, m, n = 0, 0, 0   
                clf = svm.SVC(C=C_value, kernel=kernel)
                clf.fit(kfold_PCA_train_df, kfold_train_labels)
                test_predict = clf.predict(kfold_PCA_validation_df)
                # accuracy_array[i, j, k, :, :, :] = 0
                metrics_matrix[i, j, k, l, m, n] = metrics.f1_score(kfold_test_labels, test_predict, average="micro")
                
                # Only append for fold 0
                if i == 0:
                    hyperparams_list.append((C_value, kernel))

              elif kernel == 'poly':
                      
                for l, gamma_value in enumerate(gamma_list):
                    for m, coef0_value in enumerate(coef0_list):
                        for n, deg_value in enumerate(deg_list):
                          # print(f"Working on {j} {k} {l} {m} {n}")
                          clf = svm.SVC(C=C_value, kernel=kernel, gamma=gamma_value, coef0=coef0_value, degree=deg_value)
                          clf.fit(kfold_PCA_train_df, kfold_train_labels)
                          test_predict = clf.predict(kfold_PCA_validation_df)
                          metrics_matrix[i, j, k, l, m, n] = metrics.f1_score(kfold_test_labels, test_predict, average="micro")
                          
                          if i == 0:
                              hyperparams_list.append((C_value, kernel, gamma_value, coef0_value, deg_value))

              elif kernel == 'sigmoid': 
                  
                for l, gamma_value in enumerate(gamma_list):
                    for m, coef0_value in enumerate(coef0_list):    
                        n = 0
                        clf = svm.SVC(C=C_value, kernel=kernel, gamma=gamma_value, coef0=coef0_value)
                        clf.fit(kfold_PCA_train_df, kfold_train_labels)
                        test_predict = clf.predict(kfold_PCA_validation_df)
                        metrics_matrix[i, j, k, l, m, n] = metrics.f1_score(kfold_test_labels, test_predict, average="micro")   

                        if i == 0:
                            hyperparams_list.append((C_value, kernel, gamma_value, coef0_value))

              elif kernel == 'rbf':

                for l, gamma_value in enumerate(gamma_list):
                    m, n = 0, 0
                    clf = svm.SVC(C=C_value, kernel=kernel, gamma=gamma_value)
                    clf.fit(kfold_PCA_train_df, kfold_train_labels)
                    test_predict = clf.predict(kfold_PCA_validation_df)
                    metrics_matrix[i, j, k, l, m, n] = metrics.f1_score(kfold_test_labels, test_predict, average="micro")  

                    if i == 0:
                        hyperparams_list.append((C_value, kernel, gamma_value))

      print(f"Created {len(hyperparams_list) * num_folds} SVM classifiers.")
      print("\n")

      # Exhaustive grid search: calculate which hyperparams gives highest score = max|mean - std|
      for j in range(len(C_list)):
          for k in range(len(kernel_types)):
              for l in range(len(gamma_list)):
                  for m in range(len(coef0_list)):
                      for n in range(len(deg_list)):
                          metrics_matrix_mean[j, k, l, m, n] = metrics_matrix[:, j, k, l, m, n].mean()
                          metrics_matrix_std[j, k, l, m, n] = metrics_matrix[:, j, k, l, m, n].std()

      score_matrix = metrics_matrix_mean - metrics_matrix_std

      # Find location and value of highest score
      max_value_index = np.argmax(score_matrix)
      max_value = np.max(score_matrix)
      multi_dim_index = np.unravel_index(max_value_index, score_matrix.shape)
      # print(score_array.shape)
      # print(best_param)
      # print(len(multi_dim_index))

      # Unknown error !!!!!!!!!!!!!!! ^ must be investigated

      '''
      Tror problemet ligger i at score_array er en cube med mange 0 verdier
      0 for alle verdier som ikke settes i loopen (degree 2,3,4 for 'linear' fr.eks)
      Potensielt bevis: score_array uten alle 0 verdier er like lang som hyper_param_list
      Videre gir multi_dim_index og hyper_param_list(best_param_test) like parametre
      '''
      # score_array_test = score_matrix.flatten()
      # score_array_test = [i for i in score_array_test if i != 0]
      # # print(score_array_1D)
      # print(f"Size of score_array_test: {len(score_array_test)}")
      # # Find location and value of highest score
      # best_param = np.argmax(score_matrix)
      # print(f"Index of best parameter, converted to 2D array (cube): {best_param}")
      # best_param_test = np.argmax(score_array_test) 
      # print(f"Index of best parameter, converted to 2D array (not cube): {best_param_test}")
      # print(f"Best combination of hyperparameters (C, kernel, gamma, coef0, degree): {hyperparams_list[best_param_test]}")
      # ''' Ser en del endringer ble gjort, legger denne her for nå, kan slettes '''


      clf_best_params = {
          "C": C_list[multi_dim_index[0]],
          "kernel": kernel_types[multi_dim_index[1]],
          "gamma": gamma_list[multi_dim_index[2]],
          "coef0": coef0_list[multi_dim_index[3]],
          "degree": deg_list[multi_dim_index[4]]
      }
        
      print(f"All combinations of hyper params: {len(hyperparams_list)}")

      clf = svm.SVC(**clf_best_params)
      clf.fit(df, labels)

      end_time = time.time()  # End timer
      elapsed_time = end_time - start_time
      
      print(f"Best model found and fitted in {elapsed_time:.4f} seconds")
      print(f"{clf_best_params} gives the parameter setting with the highest (mean-std) score: {max_value}")
      print(f"\n")

    elif method.lower() == 'gridsearchcv':

      clf = GridSearchCV(
            estimator = base_estimator,
            param_grid = param_grid,
            scoring = 'accuracy',
            cv = num_folds, 
            verbose = 0,
            n_jobs = -1
            )
      
      clf.fit(df, labels)
      clf_best_params = clf.best_params_

      end_time = time.time()  # End timer
      elapsed_time = end_time - start_time

      best_score = max( clf.cv_results_['mean_test_score'] - clf.cv_results_['std_test_score'] )

      print(f"Best model found and fitted in {elapsed_time:.4f} seconds")
      print(f"{clf.cv_results_['params'][clf.best_index_]} gives the parameter setting with the highest (mean - std): {best_score}")
      print(f"\n")

    elif method.lower() == 'halvinggridsearchcv':
     
      clf = HalvingGridSearchCV(
            estimator = base_estimator,
            param_grid = param_grid,
            factor = 2,
            scoring = 'accuracy',
            cv = num_folds,
            verbose = 0,
            n_jobs = -1
            )
      
      clf.fit(df, labels)
      clf_best_params = clf.best_params_

      end_time = time.time()  # End timer
      elapsed_time = end_time - start_time

      best_score = max( clf.cv_results_['mean_test_score'] - clf.cv_results_['std_test_score'] )

      print(f"Best model found and fitted in {elapsed_time:.4f} seconds")
      print(f"{clf.cv_results_['params'][clf.best_index_]} gives the parameter setting with the highest (mean - std): {best_score}")
      print(f"\n")

    elif method.lower() == 'bayessearchcv':
      
      hyperparams_space = {
        "C": Real(param_grid['C'][0], param_grid['C'][-1], prior="log-uniform"),  # Continuous log-scale for C
        "kernel": Categorical(["linear", "poly", "rbf", "sigmoid"]),  # Discrete choices
        "gamma": Real(param_grid['gamma'][0], param_grid['gamma'][-1], prior="log-uniform"),  # Log-uniform scale for gamma
        "coef0": Real(param_grid['coef0'][0], param_grid['coef0'][-1]),
        "degree": Integer(param_grid['degree'][0], param_grid['degree'][-1])
      }

      clf = BayesSearchCV(
            estimator = base_estimator,
            search_spaces = hyperparams_space,
            n_iter = 30,
            scoring = 'accuracy',
            cv = num_folds,
            verbose = 0,
            n_jobs = 5
            )
      
      clf.fit(df, labels)
      clf_best_params = clf.best_params_

      end_time = time.time()  # End timer
      elapsed_time = end_time - start_time

      best_score = max( clf.cv_results_['mean_test_score'] - clf.cv_results_['std_test_score'] )

      print(f"Best model found and fitted in {elapsed_time:.4f} seconds")
      print(f"{clf.cv_results_['params'][clf.best_index_]} gives the parameter setting with the highest (mean - std): {best_score}")
      print(f"\n")

    elif method.lower() == 'randomizedsearchcv':
      
      clf = RandomizedSearchCV(
            estimator = base_estimator,
            param_distributions = param_grid,
            n_iter = 30,
            scoring = 'accuracy',
            cv = num_folds,
            verbose = 0,
            n_jobs = 5
            )
      
      clf.fit(df, labels)
      clf_best_params = clf.best_params_

      end_time = time.time()  # End timer
      elapsed_time = end_time - start_time

      best_score = max( clf.cv_results_['mean_test_score'] - clf.cv_results_['std_test_score'] )

      print(f"Best model found and fitted in {elapsed_time:.4f} seconds")
      print(f"{clf.cv_results_['params'][clf.best_index_]} gives the parameter setting with the highest (mean - std): {best_score}")
      print(f"\n")
      
    else: 
      
      clf = base_estimator
      clf.fit(df, labels)
      clf_best_params = {'C':1, 'kernel': 'rbf'}

      end_time = time.time()  # End timer
      elapsed_time = end_time - start_time

      print(f"Base model fitted in {elapsed_time:.4f} seconds")
      print(f"Optimizer {method} not recognized, using default Support Vector Classifier.")
      print(f"\n")

    return clf, clf_best_params

def makeRFClassifier(method, base_estimator, num_folds, param_grid, df, labels):
  
  print()
  print(f"Classifier: \t {base_estimator}")
  print(f"Optimalizer: \t {method}")
  print("-" * 40)
  
  start_time = time.time()
  
  if method == 'GridSearchCV':
    
    clf = GridSearchCV(estimator=base_estimator,
                       param_grid=param_grid,
                       cv=num_folds,
                       n_jobs=-1
                       ) 
    
    clf.fit(df, labels)
    clf_best_params = clf.best_params_

    end_time = time.time()  # End timer
    elapsed_time = end_time - start_time

    best_score = max( clf.cv_results_['mean_test_score'] - clf.cv_results_['std_test_score'] )

    print(f"Best model found and fitted in {elapsed_time:.4f} seconds")
    print(f"{clf.cv_results_['params'][clf.best_index_]} gives the parameter setting with the highest (mean - std): {best_score}")
    print(f"\n")

  elif method == 'HalvingGridSearchCV':

    clf = HalvingGridSearchCV(
          estimator=base_estimator,
          param_grid = param_grid,
          cv=num_folds,
          n_jobs=-1
          )
    
    clf.fit(df, labels)
    clf_best_params = clf.best_params_
    
    end_time = time.time()  # End timer
    elapsed_time = end_time - start_time

    best_score = max( clf.cv_results_['mean_test_score'] - clf.cv_results_['std_test_score'] )

    print(f"Best model found and fitted in {elapsed_time:.4f} seconds")
    print(f"{clf.cv_results_['params'][clf.best_index_]} gives the parameter setting with the highest (mean - std): {best_score}")
    print(f"\n")

  else:
    
    clf = base_estimator

    clf.fit(df, labels)
    clf_best_params = {'n_estimators': 100,
                        'criterion': 'gini',
                        'max_depth': None,
                        'min_samples_split': 2,
                        'min_samples_leaf': 1,
                        'max_features': 'sqrt'}
  
    end_time = time.time()  # End timer
    elapsed_time = end_time - start_time

    print(f"Base model fitted in {elapsed_time:.4f} seconds")
    print(f"Method {method} not recognized, fitting default Random Forest Classifier")
    print(f"\n")

  return clf, clf_best_params

def makeKNNClassifier(method, base_estimator, num_folds, param_grid, df, labels):
    
    print()
    print(f"Classifier: \t {base_estimator}")
    print(f"Optimalizer: \t {method}")
    print("-" * 40)

    start_time = time.time()

    if method.lower() == 'gridsearchcv':
        
        clf = GridSearchCV(
            estimator=base_estimator,
            param_grid=param_grid,
            cv=num_folds,
            n_jobs=-1
            ) 
        
        clf.fit(df, labels)
        clf_best_params = clf.best_params_

        end_time = time.time()  # End timer
        elapsed_time = end_time - start_time

        best_score = max( clf.cv_results_['mean_test_score'] - clf.cv_results_['std_test_score'] )

        print(f"Best model found and fitted in {elapsed_time:.4f} seconds")
        print(f"{clf.cv_results_['params'][clf.best_index_]} gives the parameter setting with the highest (mean - std): {best_score}")
        print(f"\n")

    elif method.lower() == 'halvinggridsearchcv':
        
        clf = HalvingGridSearchCV(
            estimator=base_estimator,
            param_grid=param_grid,
            cv=num_folds,
            n_jobs=-1
            ) 
        
        clf.fit(df, labels)
        clf_best_params = clf.best_params_

        end_time = time.time()  # End timer
        elapsed_time = end_time - start_time

        best_score = max( clf.cv_results_['mean_test_score'] - clf.cv_results_['std_test_score'] )

        print(f"Best model found and fitted in {elapsed_time:.4f} seconds")
        print(f"{clf.cv_results_['params'][clf.best_index_]} gives the parameter setting with the highest (mean - std): {best_score}")
        print(f"\n")

    elif method.lower() == 'randomizedsearchcv':
        
        clf = RandomizedSearchCV(
            estimator = base_estimator,
            param_distributions = param_grid,
            n_iter = 30,
            scoring = 'accuracy',
            cv = num_folds,
            verbose = 0,
            n_jobs = -1
            )
        
        clf.fit(df, labels)
        clf_best_params = clf.best_params_

        end_time = time.time()  # End timer
        elapsed_time = end_time - start_time

        best_score = max( clf.cv_results_['mean_test_score'] - clf.cv_results_['std_test_score'] )

        print(f"Best model found and fitted in {elapsed_time:.4f} seconds")
        print(f"{clf.cv_results_['params'][clf.best_index_]} gives the parameter setting with the highest (mean - std): {best_score}")
        print(f"\n")

    else:
        
        clf = KNeighborsClassifier(n_neighbors=3)

        clf.fit(df, labels)
        clf_best_params = {"n_neighbors": 3}

        end_time = time.time()  # End timer
        elapsed_time = end_time - start_time

        print(f"Base model fitted in {elapsed_time:.4f} seconds")
        print(f"Optimizer {method} not recognized, fitting default KNN model with 3 neighbors")
        print(f"\n")
    
    return clf, clf_best_params

def makeGNBClassifier(method, base_estimator, num_folds, param_grid, df, labels):
    
    print()
    print(f"Classifier: \t {base_estimator}")
    print(f"Optimalizer: \t {method}")
    print("-" * 40)
    
    if (method == "ahadhaidiahodihaj"):
        print("HOW?!")
    else:
        clf = base_estimator
        clf.fit(df, labels)

        print(clf.get_params())
    return clf

def evaluateCLF(name, clf, test_df, test_labels, want_plots, activity_name, clf_name):
    
    print(f"{name} scores")

    test_predict = clf.predict(test_df)

    accuracy_score = metrics.accuracy_score(test_labels, test_predict)
    precision_score = metrics.precision_score(test_labels, test_predict, average="weighted")
    recall_score = metrics.recall_score(test_labels, test_predict, average="weighted")
    f1_score = metrics.f1_score(test_labels, test_predict, average="weighted")

    print(f"Accuracy: \t {accuracy_score:.4f}")
    print(f"Precision: \t {precision_score:.4f}")
    print(f"Recall: \t {recall_score:.4f}")
    print(f"f1: \t\t {f1_score:.4f}")
    print("-" * 23)

    if(want_plots):
        ''' CONFUSION MATRIX '''
        conf_matrix = metrics.confusion_matrix(test_labels, test_predict, labels=activity_name)
        plt.figure(figsize=(10, 8))
        sns.heatmap(conf_matrix, annot=True, cmap='coolwarm', xticklabels=activity_name, yticklabels=activity_name)
        plt.xlabel("Predicted")
        plt.ylabel("Actual")
        plt.title(f'Confusion matrix, {clf_name}, {name}')
    
    return accuracy_score

def makeClassifier(base_estimator, param_grid, method, X, y, search_kwargs,  n_iter=30):
    
    print()
    print(f"Classifier: \t {base_estimator}")
    print(f"Optimalizer: \t {method}")
    print("-" * 40)

    start_time = time.time()

    if method.lower() == 'gridsearchcv':
        
        clf = GridSearchCV(
           
            estimator=base_estimator,
            param_grid=param_grid,
            
            **search_kwargs

            ) 

    elif method.lower() == 'halvinggridsearchcv':
        
        clf = HalvingGridSearchCV(
           
            estimator=base_estimator,
            param_grid=param_grid,
           
            **search_kwargs

            ) 
         
    elif method.lower() == 'randomizedsearchcv':
        
        clf = RandomizedSearchCV(
           
            estimator=base_estimator,
            param_distributions=param_grid,
            
            n_iter=n_iter,
            **search_kwargs

            ) 
        
    elif method.lower() == 'bayessearchcv':
        
        smooth_param_grid = makeSmoothParamGrid(param_grid)

        clf = BayesSearchCV(

            estimator=base_estimator,
            search_spaces=smooth_param_grid,
          
            n_iter=n_iter,
            **search_kwargs

            )

    else:
       clf = base_estimator
       best_params = None
       print(f"{method} not recognized, fitting default {base_estimator}")

    clf.fit(X, y)

    end_time = time.time()  # End timer
    elapsed_time = end_time - start_time

    if clf != base_estimator: 
      best_params = clf.best_params_

      best_score = ( clf.cv_results_['mean_test_score'][clf.best_index_] - clf.cv_results_['std_test_score'][clf.best_index_] )
      print(clf.best_score_)
      print(f"{clf.cv_results_['params'][clf.best_index_]} gives the parameter setting with the highest (mean - std): {best_score}")
      print(f"Best model found and fitted in {elapsed_time:.4f} seconds")
      print(f"\n")  

    return clf, best_params

####################### from plotting.py #######################

def plot_SVM_boundaries(X, train_labels, label_mapping,
                        classifiers, optimization_methods, best_clf_params, accuracy_list):

  # Check if there is actually 2 components in the clf
  if classifiers[0].n_features_in_ == 2:

    mapped_labels = np.array([label_mapping[label] for label in train_labels])

    xs, ys = X[0], X[1]
    
    fig, sub = plt.subplots(2, 2)
    plt.subplots_adjust(wspace=0.4, hspace=0.4)

    for clf, method, title, accuracy, ax in zip(classifiers, optimization_methods, best_clf_params, accuracy_list, sub.flatten()):
        
        disp = DecisionBoundaryDisplay.from_estimator(
            clf,
            X,
            response_method="predict",
            cmap=plt.cm.coolwarm,
            alpha=0.8,
            ax=ax,
            xlabel='PC1',
            ylabel='PC2',
        )
        ax.scatter(xs, ys, c=mapped_labels, cmap=plt.cm.coolwarm, s=20, edgecolors="k")
        ax.set_xticks(())
        ax.set_yticks(())
        ax.set_title(str(method) + "\n" + "Accuracy: " + str(accuracy) + "\n" + str(title) )

  else:
    print(f"Cannot plot SVM boundaries: Classifiers has {classifiers[0].n_features_in_} features, must be 2.")

def old_biplot(X, trainLabels, PCATest, n_components, separate_types, models, optimization_methods, titles, accuracy_list):
  
  # OLD BIPLOT FUNCTION, NOT IN USE
  # PCA_object = PCA(n_components = n_components)

  if n_components == 2:

    coeff = PCATest.components_.T
    labels = PCATest.feature_names_in_

    # loadings = PCATest.components_.T * np.sqrt(PCATest.explained_variance_)
    # plt.figure(figsize=(10, 8))
    # sns.heatmap(loadings, annot=True, cmap='coolwarm', xticklabels=['PC1', 'PC2'], yticklabels=PCATest.feature_names_in_)
    # plt.title('Feature Importance in Principal Components')
    
    xs, ys = X[0], X[1]

    plt.figure(figsize=(10, 8))

    if(separate_types):
      label_mapping = {'IDLE': (0.0, 0.0, 0.0)  , 
                       'GRINDBIG': (1.0, 0.0, 0.0),'GRINDMED': (1.0, 0.5, 0.0), 'GRINDSMALL': (1.0, 0.0, 0.5),
                       'SANDSIM': (0.0, 1.0, 0.0), 
                       'WELDALTIG': (0.0, 0.0, 1.0), 'WELDSTMAG': (0.5, 0.0, 1.0), 'WELDSTTIG': (0.0, 0.5, 1.0)}
    else:
      label_mapping = {'IDLE': (0.0, 0.0, 0.0)  , 'GRINDING': (1.0, 0.0, 0.0), 'SANDSIMULATED': (0.0, 1.0, 0.0), 'WELDING': (0.0, 0.0, 1.0)}

    y_labels = np.array(trainLabels)
    mappedLabels = np.array([label_mapping[label] for label in trainLabels])

    plt.scatter(xs, ys, c=mappedLabels#, cmap='viridis'
                )
    
    
    # Check if there is actually 2 components in the clf
    if models[0].n_features_in_ == 2:

      fig, sub = plt.subplots(2, 2)
      plt.subplots_adjust(wspace=0.4, hspace=0.4)

      for clf, method, title, accuracy, ax in zip(models, optimization_methods, titles, accuracy_list, sub.flatten()):
          
          disp = DecisionBoundaryDisplay.from_estimator(
              clf,
              X,
              response_method="predict",
              cmap=plt.cm.coolwarm,
              alpha=0.8,
              ax=ax,
              xlabel='PC1',
              ylabel='PC2',
          )
          ax.scatter(xs, ys, c=mappedLabels, cmap=plt.cm.coolwarm, s=20, edgecolors="k")
          ax.set_xticks(())
          ax.set_yticks(())
          ax.set_title(str(method) + "\n" + "Accuracy: " + str(accuracy) + "\n" + str(title) )


    # Uncomment if you want arrows
    # for i in range(len(coeff)):
    #     plt.arrow(0, 0, coeff[i, 0], coeff[i, 1], color='r', alpha=0.5)
    #     plt.text(coeff[i, 0] * 1.2, coeff[i, 1] * 1.2, labels[i], color='g')

    plt.xlabel("PC1")
    plt.ylabel("PC2")
    # plt.title("Biplot")
    # plt.figure()

  elif n_components == 3:

    coeff = PCATest.components_.T
    labels = PCATest.feature_names_in_

    # loadings = PCATest.components_.T * np.sqrt(PCATest.explained_variance_)
    # plt.figure(figsize=(10, 8))
    # sns.heatmap(loadings, annot=True, cmap='coolwarm', xticklabels=['PC1', 'PC2', 'PC3'], yticklabels=PCATest.feature_names_in_)
    # plt.title('Feature Importance in Principal Components')

    xs, ys, zs = X[0], X[1], X[2]

    # Create a 3D plot
    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')
    if (separate_types):
      label_mapping = {'IDLE': (0.0, 0.0, 0.0)  , 
                       'GRINDBIG': (1.0, 0.0, 0.0),'GRINDMED': (1.0, 0.5, 0.0), 'GRINDSMALL': (1.0, 0.0, 0.5),
                       'SANDSIM': (0.0, 1.0, 0.0), 
                       'WELDALTIG': (0.0, 0.0, 1.0), 'WELDSTMAG': (0.5, 0.0, 1.0), 'WELDSTTIG': (0.0, 0.5, 1.0)}
    else:
      label_mapping = {'IDLE': (0.0, 0.0, 0.0)  , 'GRINDING': (1.0, 0.0, 0.0), 'SANDSIMULATED': (0.0, 1.0, 0.0), 'WELDING': (0.0, 0.0, 1.0)}
    y_labels = np.array(trainLabels)
    mappedLabels = np.array([label_mapping[label] for label in trainLabels])

    # Create 3D scatter plot
    sc = ax.scatter(xs, ys, zs, c=mappedLabels#, cmap='inferno'
                    )

    # Draw arrows for the components
    # for i in range(len(coeff)):
    #     ax.quiver(0, 0, 0, coeff[i, 0], coeff[i, 1], coeff[i, 2], color='r', alpha=0.5)

    #     ax.text(coeff[i, 0] * 1.2, coeff[i, 1] * 1.2, coeff[i, 2] * 1.2, labels[i], color='g')

    ax.set_xlabel('PC1')
    ax.set_ylabel('PC2')
    ax.set_zlabel('PC3')
    ax.set_title("3D Biplot")
    # plt.figure()

  elif 3 < n_components < 10:

    coeff = PCATest.components_.T
    labels = PCATest.feature_names_in_

    loadings = PCATest.components_.T * np.sqrt(PCATest.explained_variance_)

    loadingsPerc = (loadings - np.min(loadings)) / (np.max(loadings) - np.min(loadings))
    plt.figure(figsize=(10, 8))
    sns.heatmap(loadingsPerc, annot=True, cmap='coolwarm', xticklabels=['PC1', 'PC2'], yticklabels=PCATest.feature_names_in_)
    plt.title('Feature Importance in Principal Components')

  else:
    print(f"Too many principal components to plot in a meaningful way")
    pass

def plotKNNboundries(df, clf, labels):
  
    _, ax = plt.subplots()

    disp = DecisionBoundaryDisplay.from_estimator(
    clf,
    df,
    response_method="predict",
    plot_method="pcolormesh",
    shading="auto",
    alpha=0.5,
    ax=ax,
    )
    scatter = disp.ax_.scatter(df.iloc[:, 0], df.iloc[:, 1], c=labels, edgecolors="k")

########################## from realtime.py ################################

async def data_notification_handlerOLD(sender: int, data: bytearray):
    """Decode data"""
    global sample_list, notification_counter, sample_counter
    header_offset = 8   # Ignore part of notification that is header data (8 bytes)

    for k in range(DATA_BUFFER_SIZE):
        current_packet = bytearray(DATA_SIZE)                                                           # Define size of first packet in notification
        current_packet[:] = data[header_offset : header_offset + DATA_SIZE + 1]                         # Get packet data
        temp_data = Muse_Utils.DecodePacket(                                                            # Decode packet to covert values and store them in MuseData Object
            current_packet, 0, stream_mode.value, 
            gyrConfig.Sensitivity, axlConfig.Sensitivity, magConfig.Sensitivity, hdrConfig.Sensitivity
        )
        
        if(k+sample_counter >= 4000):                                           # If samples number is higher than allowed, reset sample
            print(f"Error sample counter = {k+sample_counter}, resetting. {time.strftime("%a, %d %b %Y %H:%M:%S +0000", time.localtime())}")
            sample_counter = 0
        else:                                                                   # Store data in predefined sample_list
            sample_list[k+(sample_counter)][0]  = time.time()                   # sample_list[row][column]
            sample_list[k+(sample_counter)][1]  = temp_data.axl[0]
            sample_list[k+(sample_counter)][2]  = temp_data.axl[1]
            sample_list[k+(sample_counter)][3]  = temp_data.axl[2]
            sample_list[k+(sample_counter)][4]  = temp_data.gyr[0]
            sample_list[k+(sample_counter)][5]  = temp_data.gyr[1]
            sample_list[k+(sample_counter)][6]  = temp_data.gyr[2]
            sample_list[k+(sample_counter)][7]  = temp_data.mag[0]
            sample_list[k+(sample_counter)][8]  = temp_data.mag[1]
            sample_list[k+(sample_counter)][9]  = temp_data.mag[2]
            sample_list[k+(sample_counter)][10] = temp_data.tp[0]
            sample_list[k+(sample_counter)][11] = temp_data.tp[1]
            sample_list[k+(sample_counter)][12] = temp_data.light.range
            sample_list[k+(sample_counter)][13] = temp_data.light.lum_vis
            sample_list[k+(sample_counter)][14] = temp_data.light.lum_ir

    sample_counter += DATA_BUFFER_SIZE                                          # Increase sample counter for samples included in notification

    if (sample_counter >= window_length_sec*fs):                                                        # When window_length time in seconds has passed
        ''' FEATURE EXTRACTION AND SCALE '''
        feature_df = pd.DataFrame(data=sample_list, columns=columns)                                    # Convert samples_list into dataframe to make it usable in extractFeaturesFromDF
        feature_df_extraction, label = extractFeaturesFromDF(feature_df, "Realtime", window_length_sec, fs, False)
        feature_df_scaled = scaler.transform(pd.DataFrame(feature_df_extraction))                       # Scale data with scaled from training data
       
        ''' PCA AND PREDICT '''
        PCA_feature_df = pd.DataFrame(PCA_final.transform(feature_df_scaled))                           # Convert to PC found from training data
        prediction = clf.predict(PCA_feature_df)                                                        # Predict label using classifier
        print(prediction)                                                           
        prediction_list[time.strftime("%a, %d %b %Y %H:%M:%S +0000", time.localtime())] = prediction[0] # Add prediction to prediction list dict, with timemark as reference

        sample_counter = 0
        prediction_counter += 1

    notification_counter += 1
    return

################# from preprocessing.py ##############################
def downsample(df:          pd.DataFrame,
               fs:          int,
               ds_fs:       int,
               variables:   list[str]
               ) -> pd.DataFrame:
 ''' OLD
    dropped_rows = []

    if((old_fs / new_fs).is_integer() == False):
        print(f"Old fs: {old_fs} / New fs: {new_fs} is not whole number")
        quit()
    elif((fs < ds_fs)):
        print(f"Old fs: {old_fs} is smaller than New fs: {new_fs}")
        quit()
    else:
        for i in range(len(df['Timestamp'])):
            if((i % (old_fs / new_fs)) != 0):
                dropped_rows.append(i)

    new_df = df.drop(dropped_rows)
    '''


########################## from ExtractIMU_Features.py ################################

def ExtractIMU_Features(imu_data, WindowLength, Norm_Accel):

    # Extract acceleration and gyroscope data from the IMU dataset
    time_data = imu_data[:, 0]  # Assuming 1st column is time
    accel_X = imu_data[:, 1]  # Acceleration in X direction
    accel_Y = imu_data[:, 2]  # Acceleration in Y direction
    accel_Z = imu_data[:, 3]  # Acceleration in Z direction
    gyro_X  = imu_data[:, 4]  # Gyroscope in X direction
    gyro_Y  = imu_data[:, 5]  # Gyroscope in Y direction
    gyro_Z  = imu_data[:, 6]  # Gyroscope in Z direction


    # Define a list to store features for each window
    all_window_features = []

    # Calculate the number of windows
    num_samples = len(time_data) # Number of measurements
    num_windows = num_samples // WindowLength
    # 42 = 84 322 // 2 000

    # Trying to remove first and last 10 seconds from the IMU_data sets
    # To avoid time wasted during start and stop of tests

    # cutLength = (10 * Fs) // WindowLength
    # 4 = 10 * 800 // 2 000

    print(f"Number of IMU  windows: {num_windows}")

    # for i in range(cutLength, num_windows - cutLength):

    for i in range(num_windows):
        
        # Define the start and end index for the window
        start_idx = i * WindowLength
        end_idx = start_idx + WindowLength
        print(start_idx, end_idx)  
                
        # Windowing the signals
        window_gyro_X  = gyro_X[start_idx:end_idx]
        window_gyro_Y  = gyro_Y[start_idx:end_idx]
        window_gyro_Z  = gyro_Z[start_idx:end_idx]

        if Norm_Accel == True:
            # Extract acceleration and gyroscope data from the IMU dataset
            norm_acceleration = np.sqrt( np.power(accel_X, 2) + np.power(accel_Y, 2) + np.power(accel_Z, 2))

            g_constant = np.mean(norm_acceleration)
            # print(f"g constant: {g_constant}")
            gravless_norm = np.subtract(norm_acceleration, g_constant)  
            window_accel_Norm = gravless_norm[start_idx:end_idx]

            
            window_features_accel_Norm_Time = get_Time_Domain_features_of_signal(window_accel_Norm, "accel_Norm")
            window_features_gyro_X_Time     = get_Time_Domain_features_of_signal(window_gyro_X, "gyro_X")
            window_features_gyro_Y_Time     = get_Time_Domain_features_of_signal(window_gyro_Y, "gyro_Y")
            window_features_gyro_Z_Time     = get_Time_Domain_features_of_signal(window_gyro_Z, "gyro_Z")

            window_features_accel_Norm_Freq = get_Freq_Domain_features_of_signal(window_accel_Norm, "accel_Norm", Fs=200)
            window_features_gyro_X_Freq     = get_Freq_Domain_features_of_signal(window_gyro_X, "gyro_X", Fs=200)
            window_features_gyro_Y_Freq     = get_Freq_Domain_features_of_signal(window_gyro_Y, "gyro_Y", Fs=200)
            window_features_gyro_Z_Freq     = get_Freq_Domain_features_of_signal(window_gyro_Z, "gyro_Z", Fs=200)

            ## merge all
            window_features = {**window_features_accel_Norm_Time, 
                               **window_features_accel_Norm_Freq,

                               **window_features_gyro_X_Time,
                               **window_features_gyro_Y_Time,
                               **window_features_gyro_Z_Time,
                               **window_features_gyro_X_Freq,
                               **window_features_gyro_Y_Freq,
                               **window_features_gyro_Z_Freq}
            

        if Norm_Accel == False:
            window_accel_X = accel_X[start_idx:end_idx]
            window_accel_Y = accel_Y[start_idx:end_idx]
            window_accel_Z = accel_Z[start_idx:end_idx]

            window_features_accel_X_Time = get_Time_Domain_features_of_signal(window_accel_X, "accel_X")
            window_features_accel_Y_Time = get_Time_Domain_features_of_signal(window_accel_Y, "accel_Y")
            window_features_accel_Z_Time = get_Time_Domain_features_of_signal(window_accel_Z, "accel_Z")

            window_features_gyro_X_Time  = get_Time_Domain_features_of_signal(window_gyro_X, "gyro_X")
            window_features_gyro_Y_Time  = get_Time_Domain_features_of_signal(window_gyro_Y, "gyro_Y")
            window_features_gyro_Z_Time  = get_Time_Domain_features_of_signal(window_gyro_Z, "gyro_Z")

            window_features_accel_X_Freq = get_Freq_Domain_features_of_signal(window_accel_X, "accel_X", Fs=200)
            window_features_accel_Y_Freq = get_Freq_Domain_features_of_signal(window_accel_Y, "accel_Y", Fs=200)
            window_features_accel_Z_Freq = get_Freq_Domain_features_of_signal(window_accel_Z, "accel_Z", Fs=200)

            window_features_gyro_X_Freq  = get_Freq_Domain_features_of_signal(window_gyro_X, "gyro_X", Fs=200)
            window_features_gyro_Y_Freq  = get_Freq_Domain_features_of_signal(window_gyro_Y, "gyro_Y", Fs=200)
            window_features_gyro_Z_Freq  = get_Freq_Domain_features_of_signal(window_gyro_Z, "gyro_Z", Fs=200)

            ## merge all
            window_features = {**window_features_accel_X_Time, 
                               **window_features_accel_Y_Time,
                               **window_features_accel_Z_Time,
                               **window_features_accel_X_Freq,
                               **window_features_accel_Y_Freq,
                               **window_features_accel_Z_Freq,

                               **window_features_gyro_X_Time,
                               **window_features_gyro_Y_Time,
                               **window_features_gyro_Z_Time,
                               **window_features_gyro_X_Freq,
                               **window_features_gyro_Y_Freq,
                               **window_features_gyro_Z_Freq}


        # Append the features of the current window to the list
        all_window_features.append(window_features)

    # Convert the list of features to a Pandas DataFrame for easy manipulation
    feature_df = pd.DataFrame(all_window_features)

    return feature_df

'''
def splitTrainingAndTest(num_of_windows):
    trainingShare = 0.9
    testShare = (1 - trainingShare)
    n = 3
    j = "Datafiles/20250226 Angle Grinder/26.02.2025 095305"

    feature_df = ExtractIMU_Features("imu_data", "WindowLength", "Norm_Accel")
    
    for i in feature_df:
        random = random.randint(0, 9)
        if random != 9:
            trainingData = i  
            name = set[j]
            label = name[n:]

            # append trainingData til df_training

        elif random == 9:
            testData = i
            # Label testData

            # append testData til df_testing

    


    # trainingData = num_of_windows * trainingShare

    # testData = num_of_windows * testShare'
'''


####################### Random ################################

# label_mapping   = {
#                     'IDLE':         (0.0, 0.0, 0.0), 
#                     'GRINDBIG':     (1.0, 0.0, 0.0), 'GRINDMED':    (1.0, 0.5, 0.0), 'GRINDSMALL':  (1.0, 0.0, 0.5),
#                     'IMPA':         (0.5, 0.5, 0.5), 
#                     'SANDSIM':      (0.0, 1.0, 0.0), 
#                     'WELDALTIG':    (0.0, 0.0, 1.0), 'WELDSTMAG':   (0.5, 0.0, 1.0), 'WELDSTTIG':   (0.0, 0.5, 1.0)
# }

# label_mapping   = {'IDLE': (0.0, 0.0, 0.0), 'GRINDING': (1.0, 0.0, 0.0), 'IMPA': (0.5, 0.5, 0.5), 'SANDSIMULATED': (0.0, 1.0, 0.0), 'WELDING': (0.0, 0.0, 1.0)}


        ## From testonfile 
        # header_lines = [
        #     f"_______________________________________________________________________________",
        #     f"Predictions from: {os.path.basename(file_to_test)}"
        # ]

        # header_df = pd.DataFrame([[line, "", "", ""] for line in header_lines],
        #                         columns=["Time", "Activity", "Probability", "Top-3"])

        ###Adding header above every prediction set
        # column_header = pd.DataFrame([["Time", "Activity", "Probability", "Top-3"]],
        #                         columns=["Time", "Activity", "Probability", "Top-3"])

        ### Adding the data together
        # df_result_all.append(header_df)
        # df_result_all.append(column_header)



